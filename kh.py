#%%
import pandas as pd
import numpy as np
from jieba import Tokenizer
from sklearn.preprocessing import LabelEncoder
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from mlxtend.preprocessing import DenseTransformer
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.model_selection import cross_val_score
from sklearn.metrics import accuracy_score



stop_words = [x.strip() for x in open('stopwords.txt')]
stop_words += '''
，
。
？
「
」
『
』
、
！
：
…
；
?
!
,
.
請問
我
如何
的
之
在
嗎
為何
是
哪裡
及
到
卻
請
哪些
可以
可否
會
可是
真
因為
如果
單位
哪個
'''.split('\n')


words = '''
亂丟
垃圾
柏油路面
被罰
低收入戶
為何
目前
'''.split('\n')

tokenizer = Tokenizer('./dict.txt.big.txt')

for word in words:
    tokenizer.add_word(word)


def read_answer(path):
    df = pd.read_csv(path, index_col=0)
    return df['地址'].to_dict()


def read_train_data(path, x_col, y_col):
    df = pd.read_csv(path, index_col=0)
    df.dropna(inplace=True)
    col_freq = \
        df[y_col].value_counts().to_frame() \
        .query('{} > 10'.format(y_col))
    df = df.query('{} in @col_freq.index'.format(y_col))
    size = col_freq[y_col][0]
    replace = True
    fn = lambda obj: pd.concat([
        obj,
        obj.loc[np.random.choice(obj.index, size - obj.index.size, replace),:]])
    df = df.groupby(y_col, as_index=False).apply(fn)
    X = df[x_col]
    X = df[x_col]
    Y = df[y_col]
    return (X, Y)


answer_data = read_answer('./高雄機關電話地址.csv')

path = './20170704_高雄市近期最常陳情FAQ.csv'
X, Y = read_train_data(path, '陳情內容', '回覆單位')

def analyzer(s):
    return [x for x in tokenizer.cut(s) if x not in stop_words]


label_encoder = LabelEncoder()
label_encoder = label_encoder
pipeline = Pipeline([
    ('vectorize', TfidfVectorizer(
        lowercase=False,
        analyzer=analyzer)),
    ('class', MultinomialNB()),
    ])

pipeline.fit(X, label_encoder.fit_transform(Y))

scores = cross_val_score(
    pipeline, X, label_encoder.fit_transform(Y), cv=10,
)

print("Accuracy: %0.2f (+/- %0.2f)" % (scores.mean(), scores.std() * 2))

#%%
import Levenshtein
import pandas as pd

# 慣例回應
def get_routine_response(x):
    # 讀取慣例庫
    df_routine_table = pd.read_csv('routine_table.csv')
    df_routine_table.fillna(value='', inplace=True) # 以免資料不等長出錯

    # 計算和各類別的距離
    df_routine_table['min_dist'] = [
        np.min([Levenshtein.distance(x ,i) for i in df_routine_table.iloc[i]])
        for i in range(df_routine_table.index.size)
    ]

    # 按照距離降冪排序
    df_routine_table.sort_values(by = 'min_dist', inplace = True)

    if(4 < df_routine_table['min_dist'].iloc[0]):
        print('距離過大，應該使用其他分類器')
        return -1
    else:
        return df_routine_table['response'].iloc[0]

# get_routine_response('這是一個嘗試讓距離過大的例句')

#%%
def predict(x):
    routine_response = get_routine_response(x)
    if(-1!=routine_response): #距離任何一類慣例夠接近
        return routine_response
    else: #距離任何一類慣例太遠，改用語料庫+機器學習   
        if not pipeline.named_steps['vectorize'].transform([x]).nnz:
            return \
                    '請換個更仔細的方式再敘述您的問題！'
        answer = label_encoder.inverse_transform(pipeline.predict([x]))[0]
        answer_text = answer + '\n' + answer_data.get(answer, '')
        return answer_text

#%%
predict('')
